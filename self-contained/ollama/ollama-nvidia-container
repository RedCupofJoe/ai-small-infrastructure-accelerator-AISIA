[Unit]
Description=Ollama (NVIDIA GPU via CDI) - Podman Quadlet
After=network-online.target
Wants=network-online.target

[Container]
Image=docker.io/ollama/ollama:latest
PublishPort=127.0.0.1:11400:11434
Volume=%h/.ollama:/root/.ollama:Z

# Expose all NVIDIA GPUs via CDI
AddDevice=nvidia.com/gpu=all

# Optional envs (Ollama autodetects CUDA when GPUs are present)
Environment=OLLAMA_HOST=0.0.0.0
Environment=OLLAMA_KEEP_ALIVE=10m
# If you want to pin BLAS threads, etc., you can add more envs here.

Label=io.containers.autoupdate=registry
Restart=on-failure
StopTimeout=60

[Service]
RestartSec=3

[Install]
WantedBy=default.target
